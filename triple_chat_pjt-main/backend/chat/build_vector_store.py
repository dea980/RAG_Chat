from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
import pandas as pd
import openai
import os
import sys
import django
from django.conf import settings
import uuid
import datetime
import logging

# Add the backend directory to Python path
backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(backend_dir)

# Set up Django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'triple_chat_pjt.settings')
django.setup()

# Import after Django setup
from .vector_metadata import VectorMetadataManager
from .utils import MetaDataManager

logger = logging.getLogger(__name__)

def load_excel_data(file_path: str):
    """
    Load data from Excel file and convert to Langchain documents
    with enhanced metadata for search optimization
    """
    # Read all sheets from Excel file
    sheets = pd.read_excel(file_path, sheet_name=None, engine="openpyxl")
    
    all_docs = []
    sheet_names = list(sheets.keys())
    second_sheet_name = sheet_names[1] if len(sheet_names) > 1 else None
    
    # Generate a batch ID for this import
    batch_id = str(uuid.uuid4())
    processing_time = datetime.datetime.now().isoformat()
    
    # Store batch metadata
    batch_metadata = {
        "source": "excel",
        "file_path": file_path,
        "sheet_count": len(sheet_names),
        "sheets": sheet_names,
        "processing_time": processing_time,
        "batch_id": batch_id
    }
    
    # Store this in the metadata system
    MetaDataManager.set(
        key=f"excel_import_{batch_id}",
        value=batch_metadata,
        description=f"Excel import metadata for batch {batch_id}"
    )
    
    # Process each sheet
    for sheet_name, df in sheets.items():
        for index, row in df.iterrows():
            # Create document text
            text = "\n".join(f"{col}: {row[col]}" for col in df.columns if pd.notna(row[col]))
            
            # Create comprehensive metadata
            metadata = {
                "sheet": sheet_name,
                "row_index": index,
                "batch_id": batch_id,
                "doc_id": f"{batch_id}_{sheet_name}_{index}",
                "source": "excel",
                "processing_time": processing_time,
                "content_length": len(text),
                "column_count": len(df.columns)
            }
            
            # Extract all non-null values as separate metadata fields
            for col in df.columns:
                if pd.notna(row[col]):
                    # Use a prefix to avoid conflicts with standard metadata
                    safe_col_name = col.replace(" ", "_").lower()
                    metadata[f"field_{safe_col_name}"] = str(row[col])
            
            # Handle image paths specially
            if sheet_name == second_sheet_name and "Image Path" in df.columns:
                image_path = row.get("Image Path", None)
                if pd.notna(image_path):
                    metadata["image_path"] = image_path
            
            # Create the document
            doc = Document(page_content=text, metadata=metadata)
            all_docs.append(doc)
    
    # Store document metadata in the separate metadata system
    logger.info(f"Storing metadata for {len(all_docs)} documents from Excel")
    VectorMetadataManager.store_vector_batch_metadata(all_docs, f"excel_{batch_id}")
    
    return all_docs

def build_vector_store():
    '''
    RAGë¥¼ êµ¬ì¶•í•˜ëŠ” ì‹œì ê³¼ RAGë¥¼ ì‚¬ìš©í•˜ëŠ” ì‹œì ì„ ë¶„ë¦¬
    âœ… ì„œë²„ êµ¬ë™ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰í•˜ê¸°
    ë©”íƒ€ë°ì´í„°ë¥¼ ë³„ë„ë¡œ ì €ì¥í•˜ì—¬ ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™”
    '''
    try:
        # Set OpenAI API key
        openai.api_key = "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"
        os.environ["OPENAI_API_KEY"] = openai.api_key
        
        # Generate an ID for this build process
        build_id = str(uuid.uuid4())
        build_time = datetime.datetime.now().isoformat()
        
        # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ì¶œë ¥
        current_dir = os.getcwd()
        logger.info(f"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {current_dir}")
        
        # CSV íŒŒì¼ ê²½ë¡œ í™•ì¸
        csv_path = os.path.join(current_dir, "galaxy_s25_data.csv")
        logger.info(f"CSV íŒŒì¼ ê²½ë¡œ: {csv_path}")
        
        # ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ê²½ë¡œ í™•ì¸
        vector_store_path = os.path.join(current_dir, "vector_store")
        logger.info(f"ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ê²½ë¡œ: {vector_store_path}")
        
        # Store build start metadata
        MetaDataManager.set(
            key="vector_build_latest",
            value={
                "build_id": build_id,
                "start_time": build_time,
                "status": "started",
                "csv_path": csv_path,
                "vector_store_path": vector_store_path
            },
            description="Latest vector store build information"
        )
        
        # 1. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
        model = ChatOpenAI(model="gpt-4o-mini")
        
        # 2. ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
        from langchain_community.document_loaders import CSVLoader
        loader = CSVLoader(file_path=csv_path)
        docs = loader.load()
        logger.info(f"ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")
        
        # Enhance documents with metadata before chunking
        enhanced_docs = []
        for i, doc in enumerate(docs):
            # Add comprehensive metadata
            doc.metadata.update({
                "doc_id": f"csv_{build_id}_{i}",
                "build_id": build_id,
                "source": "csv",
                "original_index": i,
                "content_length": len(doc.page_content),
                "processing_time": build_time
            })
            enhanced_docs.append(doc)
        
        # Store the original docs metadata
        VectorMetadataManager.store_vector_batch_metadata(
            enhanced_docs,
            f"csv_original_{build_id}"
        )
        
        # ë¬¸ì„œ chunking
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(enhanced_docs)
        logger.info(f"ì²­í¬ ìˆ˜: {len(splits)}")
        
        # Add chunk-specific metadata
        for i, chunk in enumerate(splits):
            chunk.metadata.update({
                "chunk_id": f"chunk_{build_id}_{i}",
                "chunk_index": i,
                "is_chunk": True
            })
        
        # Store the chunked docs metadata
        VectorMetadataManager.store_vector_batch_metadata(
            splits,
            f"csv_chunks_{build_id}"
        )
        
        # embedding ë„êµ¬ ì„¤ì •
        embeddings = OpenAIEmbeddings()
            
        # Chroma ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° ì €ì¥
        vector_store_path = settings.VECTOR_STORE_PATH
        logger.info(f"ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ê²½ë¡œ: {vector_store_path}")
        
        vector_store = Chroma.from_documents(
            documents=splits,
            embedding=embeddings,
            persist_directory=vector_store_path
        )
        
        # ì €ì¥ í™•ì¸
        doc_count = vector_store._collection.count()
        logger.info(f"ë²¡í„° ìŠ¤í† ì–´ ë¬¸ì„œ ìˆ˜: {doc_count}")
        logger.info(f"ğŸ“‚ ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ê²½ë¡œ: {vector_store._persist_directory}")
        
        # Update build metadata with success status
        MetaDataManager.set(
            key="vector_build_latest",
            value={
                "build_id": build_id,
                "start_time": build_time,
                "end_time": datetime.datetime.now().isoformat(),
                "status": "completed",
                "document_count": doc_count,
                "chunk_count": len(splits),
                "vector_store_path": vector_store._persist_directory
            },
            description="Latest vector store build information"
        )
        
        # Create search index metadata
        VectorMetadataManager.create_search_index(
            "default",
            {
                "build_id": build_id,
                "document_count": doc_count,
                "last_updated": datetime.datetime.now().isoformat(),
                "embedding_model": "text-embedding-ada-002",
                "chunk_size": 1000,
                "chunk_overlap": 200
            }
        )
        
        return {
            "status": "success",
            "build_id": build_id,
            "document_count": doc_count
        }
        
    except Exception as e:
        error_msg = str(e)
        logger.error(f"ì—ëŸ¬ ë°œìƒ: {error_msg}")
        
        # Update build metadata with failure status
        MetaDataManager.set(
            key="vector_build_latest",
            value={
                "build_id": build_id if 'build_id' in locals() else "unknown",
                "status": "failed",
                "error": error_msg,
                "time": datetime.datetime.now().isoformat()
            },
            description="Latest vector store build information"
        )
        
        return {
            "status": "error",
            "error": error_msg
        }

# í•´ë‹¹ ìŠ¤í¬ë¦½íŠ¸ê°€ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ build_vector_store() í•¨ìˆ˜ë¥¼ ì‹¤í–‰
if __name__ == "__main__":
    '''
    ì´ íŒŒì¼ì´ ë…ë¦½ì ì¸ ì‹¤í–‰ íŒŒì¼ë¡œ ì‹¤í–‰ë  ë•Œë§Œ build_vector_store()ê°€ ì‹¤í–‰ë˜ê³ ,
    ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ importë  ë•ŒëŠ” ì‹¤í–‰ë˜ì§€ ì•ŠëŠ”ë‹¤.
    '''
    # Set up logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[logging.StreamHandler()]
    )
    
    # Initialize metadata defaults
    MetaDataManager.set(
        key="current_time",
        value=datetime.datetime.now().isoformat(),
        description="Current time when the script ran"
    )
    
    # Build the vector store with metadata enhancements
    result = build_vector_store()
    
    if result['status'] == 'success':
        logger.info(f"Vector store build completed successfully. Build ID: {result['build_id']}")
        logger.info(f"Total documents indexed: {result['document_count']}")
    else:
        logger.error(f"Vector store build failed: {result['error']}")